{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5GenerationModel:\n",
    "    inspiration_prefix = 'Сгенерировать вдохновение: '\n",
    "    mark_prefix = 'Сгенерировать оценку: '\n",
    "    punch_prefix = 'Сгенерировать шутку: '\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "\n",
    "    def load_model_from_file(self, model_dir):\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(model_dir)\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(model_dir)\n",
    "\n",
    "    def load_model_from_hub(self,\n",
    "                      model_name,\n",
    "                      model_type,\n",
    "                      force_download,\n",
    "                      use_auth_token,\n",
    "                      revision):\n",
    "\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(model_name,\n",
    "                                                     from_flax=model_type == \"flax\",\n",
    "                                                     force_download=force_download,\n",
    "                                                     use_auth_token=use_auth_token,\n",
    "                                                     revision=revision)\n",
    "\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(model_name,\n",
    "                                                                from_flax=model_type == \"flax\",\n",
    "                                                                force_download=force_download,\n",
    "                                                                use_auth_token=use_auth_token,\n",
    "                                                                revision=revision)\n",
    "        \n",
    "    def save_weights(self, path):\n",
    "        self.model.save_pretrained(path)\n",
    "        self.tokenizer.save_pretrained(path)\n",
    "        \n",
    "\n",
    "    def generate_inspirations(self, setup: str,\n",
    "                              num_return_sequences: int = 5, temperature: float = 1) -> List[str]:\n",
    "        # Generate inspirations\n",
    "        setup_ids = self.tokenizer(self.inspiration_prefix + setup, return_tensors=\"pt\").input_ids\n",
    "        predict_inspiration_ids = self.model.generate(setup_ids,\n",
    "                                                      top_k=20,\n",
    "                                                      do_sample=True,\n",
    "                                                      max_length=50,\n",
    "                                                      no_repeat_ngram_size=2,\n",
    "                                                      temperature=temperature,\n",
    "                                                      num_return_sequences=num_return_sequences).tolist()\n",
    "        predict_inspirations = [self.tokenizer.decode(p, skip_special_tokens=True) for p in predict_inspiration_ids]\n",
    "        return predict_inspirations\n",
    "\n",
    "    def generate_punches(self, setup: str, inspiration: str,\n",
    "                         num_return_sequences: int = 5, temperature: float = 1) -> List[str]:\n",
    "\n",
    "        input_ids = self.tokenizer(self.punch_prefix + inspiration + '|' + setup, return_tensors=\"pt\").input_ids\n",
    "        predict_punches_ids = self.model.generate(input_ids,\n",
    "                                                  do_sample=True,\n",
    "                                                  top_k=20,\n",
    "                                                  max_length=50,\n",
    "                                                  no_repeat_ngram_size=2,\n",
    "                                                  temperature=temperature,\n",
    "                                                  num_return_sequences=num_return_sequences).tolist()\n",
    "        predict_punches = [self.tokenizer.decode(p, skip_special_tokens=True) for p in predict_punches_ids]\n",
    "        return predict_punches\n",
    "\n",
    "    def generate_mark(self, joke: str) -> str:\n",
    "        input_ids = self.tokenizer(self.mark_prefix + joke, return_tensors=\"pt\").input_ids\n",
    "        predict_mark_ids = self.model.generate(input_ids).tolist()\n",
    "        predict_mark = self.tokenizer.decode(predict_mark_ids[0], skip_special_tokens=True)\n",
    "        return predict_mark\n",
    "\n",
    "    def inference(self, setup: str, inspirations: List = None,\n",
    "                  num_return_sequences: int = 5, temperature: float = 1) -> List[Tuple[str, str, str, str]]:\n",
    "        result_list = list()\n",
    "        if not inspirations:\n",
    "            # Generate inspirations\n",
    "            inspirations = self.generate_inspirations(setup,\n",
    "                                                      num_return_sequences=num_return_sequences,\n",
    "                                                      temperature=temperature, )\n",
    "        for inspiration in inspirations:\n",
    "            # Generate punches\n",
    "            punches = self.generate_punches(setup,\n",
    "                                            inspiration,\n",
    "                                            num_return_sequences=num_return_sequences,\n",
    "                                            temperature=temperature)\n",
    "            for punch in punches:\n",
    "                joke = setup + punch\n",
    "                mark = self.generate_mark(joke)\n",
    "                result_list.append((setup, inspiration, punch, mark))\n",
    "\n",
    "        sorted_result_list = sorted(result_list, key=lambda tup: tup[3], reverse=True)[:num_return_sequences]\n",
    "        return sorted_result_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "\n",
    "# Default sberbank model used for finetune\n",
    "# model_name, model_type = \"sberbank-ai/ruT5-large\", \"pytorch\"\n",
    "\n",
    "# Model finetuned on span masks task\n",
    "# model_name, model_type = \"naltukhov/joke-generator-t5-rus\", \"flax\"\n",
    "\n",
    "# Model finetuned on span masks task and generation task\n",
    "model_name, model_type = \"naltukhov/joke-generator-t5-rus-finetune-gen\", \"flax\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Span mask finetune model versions\n",
    "\n",
    "# 2dbd2be9074a544dddd353b4c1e2b579de99f0be # 25 -> production (sberbank-ai/ruT5-large)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gererator model versions\n",
    "\n",
    "# Last\n",
    "revision = None\n",
    "\n",
    "\n",
    "# Model with inspirations on small dataset (first success workflow)\n",
    "# For naltukhov/joke-generator-t5-rus-finetune model\n",
    "# revision = \"159b2223b230be99faa5f9d661996c757f58d66a\"  # 26\n",
    "# revision = \"9aaf18df0cb1dc5de2480222e52f504837306048\"  # 27\n",
    "\n",
    "# Model with inspirations on big dataset (the first run, overfitting?)\n",
    "# revision = \"c2a18676b1e782c4deef22e1d85a261f134f3a85\" # 36\n",
    "# revision = \"a001d2b3c44d193f489f2e3704ca13776a57a43b\" # 34\n",
    "# revision = \"5514501d62fab937ba8cd77ae17ed062fbb3bf74\" # 28\n",
    "\n",
    "# Model with inspirations on big dataset (the second run)\n",
    "revision = \"af089323b89ca9c1968f7c0f34c1d77be2d3d6d4\" # 40 -> production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5fefa0b827b4839b465a15b1b36b6de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/980k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa9fd80db40a4ce8b8dcdf8a3e435e36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.74k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25a9a4fe79484db1aa831a33c7b55ce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.90k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc44060377694c87a53fefb0ddc2d4eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.39k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4c07febd7144c2ca385d6b374aeca52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.75G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All Flax model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the Flax model and are newly initialized: ['lm_head.weight', 'decoder.embed_tokens.weight', 'encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = T5GenerationModel()\n",
    "model.load_model_from_hub(\n",
    "    model_name=model_name,\n",
    "    model_type=model_type,\n",
    "    revision=revision,\n",
    "    force_download=True,\n",
    "    use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "setup = 'Российские войска ведут штурм и наступают на двух направлениях'\n",
    "# setup = 'Медведев приехал на церемонию прощания с Горбачевым'\n",
    "# setup = 'Я плохой рассказчик'\n",
    "# setup = 'Медведь шёл по лесу'\n",
    "\n",
    "predicts = model.inference(setup, \n",
    "                           num_return_sequences=5, \n",
    "                           temperature=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'setup': 'Российские войска ведут штурм и наступают на двух направлениях',\n",
       "  'inspiration': 'сирия',\n",
       "  'punch': 'А у нас есть Сирии. А сирия - в Саратове',\n",
       "  'mark': '1'},\n",
       " {'setup': 'Российские войска ведут штурм и наступают на двух направлениях',\n",
       "  'inspiration': 'сирия',\n",
       "  'punch': 'Сирю на две сири в завязке на двух, на четыре...',\n",
       "  'mark': '1'},\n",
       " {'setup': 'Российские войска ведут штурм и наступают на двух направлениях',\n",
       "  'inspiration': 'ща',\n",
       "  'punch': 'А я как ща, в рюрик в отруби!',\n",
       "  'mark': '1'},\n",
       " {'setup': 'Российские войска ведут штурм и наступают на двух направлениях',\n",
       "  'inspiration': 'ща',\n",
       "  'punch': 'Е не надо, я в щах!',\n",
       "  'mark': '1'},\n",
       " {'setup': 'Российские войска ведут штурм и наступают на двух направлениях',\n",
       "  'inspiration': 'ща',\n",
       "  'punch': 'Ну, и ща им в завязке, ели у нас уже',\n",
       "  'mark': '1'}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts_df = pd.DataFrame(predicts, columns=['setup', 'inspiration', 'punch', 'mark'])\n",
    "predicts_df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Punch generation\n",
    "df = pd.read_csv('data/agg-generation-dataset/agg-generation-dataset-test.csv')\n",
    "df = df.loc[df.input.str.startswith(punch_prefix)]\n",
    "for i in range(10):\n",
    "    input, output = df.sample().values[0]\n",
    "    print(f'\\tExample {i + 1}:')\n",
    "\n",
    "    predicts = inference(input)\n",
    "    print(*predicts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save prod version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/naltukhov/joke-generator-rus-t5 into local empty directory.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('https://huggingface.co/naltukhov/joke-generator-rus-t5/commit/829409111cfd78e24077fe4034edb4b526c1e661',\n",
       " [push command, status code: running, in progress. PID: 42940])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from huggingface_hub import Repository\n",
    "\n",
    "dir_name = 't5_jokes_generator_v3'\n",
    "model_name = \"naltukhov/joke-generator-rus-t5\"\n",
    "\n",
    "\n",
    "if os.path.exists(dir_name):\n",
    "    shutil.rmtree(dir_name)\n",
    "\n",
    "# Create local repo\n",
    "repo = Repository(dir_name, clone_from=model_name)\n",
    "    \n",
    "# Save as pytorch model\n",
    "model.save_weights(dir_name)\n",
    "print('Saved model weights locally')\n",
    "\n",
    "# Push\n",
    "print('Start pushing model weiths to repo')\n",
    "repo.push_to_hub(commit_message=f\"Add new model version\", blocking=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8be962f80014674be57e093379966f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/980k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91946ca78a61435c8fdfdc059285cdd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.74k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6058bc2eee78481a803bad08935ed258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.93k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1970cacb3c944a27a14ff30a99727187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.43k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0f72da8a61c42389b990db85b6d2e68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.75G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[('Российские войска ведут штурм и наступают на двух направлениях',\n",
       "  'вражеский атака',\n",
       "  'Вражеская атака',\n",
       "  '1'),\n",
       " ('Российские войска ведут штурм и наступают на двух направлениях',\n",
       "  'вражеский атака',\n",
       "  'Вражеские атаки и ступаются в отпуске, в вражейский',\n",
       "  '1'),\n",
       " ('Российские войска ведут штурм и наступают на двух направлениях',\n",
       "  'вражеский атака',\n",
       "  'Вражеский атаку',\n",
       "  '1'),\n",
       " ('Российские войска ведут штурм и наступают на двух направлениях',\n",
       "  'вражеский атака',\n",
       "  'Вражеские атаки и так посылают',\n",
       "  '1'),\n",
       " ('Российские войска ведут штурм и наступают на двух направлениях',\n",
       "  'сбить',\n",
       "  'Чтобы не сбили',\n",
       "  '1')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test prod model\n",
    "model = T5GenerationModel()\n",
    "model.load_model_from_hub(model_name=\"naltukhov/joke-generator-rus-t5\",\n",
    "                          model_type=\"pytorch\",\n",
    "                          use_auth_token=False,\n",
    "                          force_download=True,\n",
    "                          revision=None)\n",
    "\n",
    "setup = 'Российские войска ведут штурм и наступают на двух направлениях'\n",
    "predicts = model.inference(setup, \n",
    "                           num_return_sequences=5, \n",
    "                           temperature=0.9)\n",
    "\n",
    "predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
